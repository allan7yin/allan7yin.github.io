### String Matching

|   |   |   |
|---|---|---|
|**Algorithm Name**|**Brief Description**|**Runtime**|
|**Karp-Rabin Algorithm**|Uses hash function to perform string matching. We only work with this algorithm when working with “integer” composition strings.  <br>  <br>  <br>**Note:** In the pseudocode, to get to the next hash value, we perform:  <br>  <br>$h_T = ((h_T - T[i]·s)·10 + T[i+m]) modM $﻿, where $s = 10^{m-1} modM$﻿. This is 10 as we are in base 10, digits 0-9. If, say, we wanted to apply this algorithm to all ASCII values, would become base 128, where we then alter s and the algorithm accordingly.  <br>  <br>  <br>**Note: The value** **$modk$**﻿ **is random prime number from** **${2,...,mn^2}$**|**Expected:** **$O(m+n)$**﻿**  <br>Worst:  <br>****$O(mn)$**|
|**Knuth-Morris-Pratt Algorithm**|This is quite the long algorithm, but simple in nature.  <br>  <br>  <br>**Description:** We want to be able to match prefixes and suffixes to reduce the amount of comparisons that are done. To do so, we build **Failure Array. It is built by essentially performing Knuth-Morris on each of the substrings P[0,…,j] of the pattern P.  <br>  <br>  <br>**Once the failure array is done, we use it. When comparing **P** and **T,** if character is equal, move on (i++, j++), if different and j≠0, then we move the to index specified by **j = F[j-1]**.|**Failure Array Computation Time:** **$O(m)$**﻿**  <br>-  <br>**This is calculated through analysis that m ≤ # of iterations ≤ 2m  <br>  <br>  <br>**Overall:** **$O(m + n)$**﻿**  <br>-  <br>**Same analysis as time complexity for failure array|
|**Boyer-Moore Algorithm**|**⇒ Fastest for English Text**  <br>This is based on the same idea as Knuth-Morris Algorithm, looking to match suffixes/prefixes by shifting the pattern to get a match. Like how Knuth-Morris had 3 situations on what to do (shift, move over one, match), we need to shift in the case of a mismatch, this is called  <br>**bad character heuristic**. We build an array called **last occurrence array**. It simply holds the largest index a character appears in the pattern. Upon mismatch, we do:  <br>  <br>  <br>**⇒ j = m - 1  <br>⇒  <br>****$i = i + m - 1 - min\{L(c), j-1\}$**﻿**  <br>  <br>Boyer-Moore seeks to match from the end of the pattern, hence why we set j = m-1. This method allows us to skip more future mismatches when a mismatch at the current position occurs.  <br>  <br>Note:  <br>****To create the last occurrence array, we look from i = 0 to m-1, and assign L[P[i]] = i.**|**Worst:** **$O(mn)$**﻿**,** but in practice, is much faster  <br>  <br>The algorithm must choose at every mismatch, whether to follow the  <br>**bad character heuristic or the good suffix (as we don’t cover good suffix, chooses between bad character and brute force)**|
|**Suffix Trees**|**Idea:** We are searching form multiple patterns. Instead of doing pre-processing on the pattern (of which there may be multiple), we perform pre-processing on the text.  <br>  <br>  <br>**Description:** We store all substrings of the text inside of a **compressed trie** → this is called suffix tree. To search, traverse the tree based on the current character in the pattern. The only difference is now our search needs to be able to stop at a non-leaf node (as the pattern may be a substring of a suffix).  <br>  <br>  <br>**Note:** Theoretically good idea, but slow + complicated construction. This is rarely used.|**Building/Preprocessing:** $O(\|alphabet\| · n^2) $﻿  <br>  <br>**Searching (if trie is LL):** **$O(\|alphabet\| · m) $**﻿  <br>  <br>**Searching (if trie is Array):** **$O(m)$**|
|**Suffix Arrays**|**Idea:** For this, will need to append **$** to end of text. Store all suffixes into an array. Then, sort lexicographically, with **$** being interpreted as the smallest value character. We do this with advanced version (we don’t learn) of MSD-Radix sort. So this is sorted in $O(nlogn) $﻿ time. Then, to search for the pattern, we perform binary search on the sorted array.|There are $O(logn) $﻿ comparisons, and comparing strings is $O(m)$﻿.  <br>  <br>  <br>**Overall:** **$O(mlogn)$**|

### Compression

|   |   |   |
|---|---|---|
|**Algorithm Name**|**Brief Description**|**Runtime**|
|**Huffman’s Algorithm**|This algorithm aims to construct a trie that can be used for encoding and decoding. We let $$﻿. To form this trie, the process is fairly straightforward. Using a frequency array, we create tries for each character with their frequency, and keep joining the 2 smallest frequency tries until we are left with a one single trie. To do this, we can either: (1) eye-ball it, which really only works on paper for assessments, (2) use a min-heap, where we call deleteMin twice, merge the two tries, and then insert it, with the combined frequency, back into the minHeap  <br>  <br>  <br>**Note:** What this does is that now, more frequently used characters have shorter encodings (hence, shorter paths from the root), and less frequently used characters have longer encodings (hence, longer paths from the root).|**Frequency Array:** **$O(n)$**﻿**  <br>  <br>Compute Trie:  <br>**$O(\|\sum_S\|log\|\sum_C\|)$﻿**  <br>Encode S:  <br>****$O(\|\sum_S\| + \|C\|)$**﻿**  <br>  <br>Total:  <br>****$O(\|\sum_S\|log\|\sum_C\| + \|C\|)$**|
|**RLE (Run Length Encoding)**|This algorithm aims to compress data where there are a lot of runs of one character, something like 11111100000111… something of that nature.  <br>  <br>The encoding and decoding details are fairly straightforward. When encoding, we want to encode the lengths of each block of 1’s or 0’s and pad k 0’s in front. When decoding, we need only to read in that number (padded by length - 1 0’s in front of it).  <br>  <br>The length of a run is  <br>$log(\lfloor k\rfloor) +(log(\lfloor k\rfloor) + 1)$﻿ for a run. For the entire  <br>  <br>  <br>**⇒ When the dictionary is uniquely defined by an algorithm, not influenced by the input (unlike adaptive ones, which then will need to be passed to decoder)**|**Decoding:** **$O(\|S\|)$**|
|**Lempel-Ziv-Welch**|Now, this compression algorithm is focused on the idea of adding to the dictionary as it reads in words. We encode letters, or sequences of letters, into numbers (that we can later convert into 12 bit code numbers).  <br>  <br>  <br>**⇒ Encoding:** We will need to begin with a dictionary that has some entries (e.g some letters). When we encounter them in S, take their encoding from the dictionary. Then, we take the current string, add the next character in S, and add that entry to the dictionary.  <br>  <br>  <br>**⇒ Decoding:** When we see a string or character, we do not get the dictionary passed from the encoding. We must build it again. Here, again, we will have some beginner entries in the dictionary. Then, for each number, we fetch the corresponding string from the dictionary. That is, we add $S_{prev} + S[0]$﻿ to the dictionary. If S is not found in the dictionary, then $S = S_{prev} + S_{prev}[0]$﻿, which we then add $S_{prev} + S[0]$﻿ to the dictionary.  <br>  <br>To save space, instead of saving the actual string there, in the dictionary, we can store the new character we are appending, and an index to where to first substring is found in the dictionary.  <br>  <br>  <br>**⇒ Works badly if no repeated substrings: Dictionary keeps growing, but nothing is being used**|**Encoding:** **$O(\|S\|)$**﻿**  <br>Decoding:  <br>**$O(\|S\|)$|
|**Bzip2**|This is a compression, that consist of 2 transformations and 2 encoding schemes. This uses the following order:  <br>  <br>**1. Burrows-Wheeler** (Transform text into a format favourable for MTF)**  <br>2. MTF Transformation  <br>**(move to front, take the dictionary with A-Z, and move to front when we encode with array index, then read next. This makes it so, if we end up decoding the same character multiple times in succession, it will become a sequence of 0’s. Good for RLE)  <br>  <br>**3. Modified RLE** (where essentially, we just have an extra character in front of a run to indicate that it is a run: 3333 → 300100  <br>  <br>**4. Huffman Encoding**||
|**Burrows-Wheeler Algorithm**|Read Final Exam Notes for details.||